{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a7d35e7-42b9-4ac4-8130-f54bd4347f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from codecarbon import EmissionsTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8c0aac",
   "metadata": {},
   "source": [
    "# **Directorio para resultados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f829fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = \"results\"\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "os.makedirs(f\"{result_path}/emissions_reports\", exist_ok=True)\n",
    "os.makedirs(f\"{result_path}/emissions_plots\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e839fc6",
   "metadata": {},
   "source": [
    "# **Parámetros de entrenamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb2f17c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'embedding_dim': 128,\n",
    "    'hidden_dim': 128,\n",
    "    'batch_size': 1024,\n",
    "    'learning_rate': 1e-3,\n",
    "    'epochs': 50,\n",
    "    'display_step': 1,\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b516e",
   "metadata": {},
   "source": [
    "# **Clase para el seguimiento de métricas del sistema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51d23541",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SystemMetricsTracker:\n",
    "    def __init__(self):\n",
    "        self.train_metrics = []\n",
    "        self.test_metrics = {}\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def start_epoch(self, epoch):\n",
    "        self.epoch_start_time = time.time()\n",
    "        self.current_epoch_metrics = {\n",
    "            'epoch': epoch,\n",
    "            'memory_usage_mb': psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2,\n",
    "            'cpu_usage_percent': psutil.cpu_percent(),\n",
    "        }\n",
    "        \n",
    "    def end_epoch(self, epoch, loss, rmse=None, mae=None):\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.current_epoch_metrics['epoch_time_sec'] = epoch_time\n",
    "        self.current_epoch_metrics['loss'] = loss\n",
    "        if rmse is not None:\n",
    "            self.current_epoch_metrics['rmse'] = rmse\n",
    "        if mae is not None:\n",
    "            self.current_epoch_metrics['mae'] = mae\n",
    "        self.train_metrics.append(self.current_epoch_metrics)\n",
    "        \n",
    "        # Imprimir resumen de época\n",
    "        print(f\"\\nEpoch {epoch} Metrics:\")\n",
    "        print(f\"  Time: {epoch_time:.2f}s\")\n",
    "        print(f\"  Memory: {self.current_epoch_metrics['memory_usage_mb']:.2f}MB\")\n",
    "        print(f\"  CPU: {self.current_epoch_metrics['cpu_usage_percent']:.1f}%\")\n",
    "        print(f\"  Loss: {loss:.4f}\")\n",
    "        if rmse is not None:\n",
    "            print(f\"  RMSE: {rmse:.4f}\")\n",
    "        if mae is not None:\n",
    "            print(f\"  MAE: {mae:.4f}\")\n",
    "        \n",
    "    def end_test(self, rmse, mae=None):\n",
    "        self.test_metrics = {\n",
    "            'test_time_sec': time.time() - self.epoch_start_time,\n",
    "            'total_time_sec': time.time() - self.start_time,\n",
    "            'final_memory_usage_mb': psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2,\n",
    "            'final_cpu_usage_percent': psutil.cpu_percent(),\n",
    "            'test_rmse': rmse,\n",
    "        }\n",
    "        if mae is not None:\n",
    "            self.test_metrics['test_mae'] = mae\n",
    "        \n",
    "        # Imprimir métricas finales\n",
    "        print(\"\\n=== Final Training Metrics ===\")\n",
    "        for m in self.train_metrics:\n",
    "            metrics_str = f\"Epoch {m['epoch']}: Time={m['epoch_time_sec']:.2f}s, Memory={m['memory_usage_mb']:.2f}MB, CPU={m['cpu_usage_percent']:.1f}%, Loss={m['loss']:.4f}\"\n",
    "            if 'rmse' in m:\n",
    "                metrics_str += f\", RMSE={m['rmse']:.4f}\"\n",
    "            if 'mae' in m:\n",
    "                metrics_str += f\", MAE={m['mae']:.4f}\"\n",
    "            print(metrics_str)\n",
    "        \n",
    "        print(\"\\n=== Final Test Metrics ===\")\n",
    "        print(f\"Total Time: {self.test_metrics['total_time_sec']:.2f}s (Test: {self.test_metrics['test_time_sec']:.2f}s)\")\n",
    "        print(f\"Final Memory: {self.test_metrics['final_memory_usage_mb']:.2f}MB\")\n",
    "        print(f\"Final CPU: {self.test_metrics['final_cpu_usage_percent']:.1f}%\")\n",
    "        print(f\"Test RMSE: {rmse:.4f}\")\n",
    "        if mae is not None:\n",
    "            print(f\"Test MAE: {mae:.4f}\")\n",
    "        \n",
    "        # Guardar métricas en CSV\n",
    "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        metrics_df = pd.DataFrame(self.train_metrics)\n",
    "        metrics_df.to_csv(f\"{result_path}/system_metrics_{timestamp}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd40c273",
   "metadata": {},
   "source": [
    "# **Clase para seguimiento de emisiones por época**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69093f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmissionsPerEpochTracker:\n",
    "    def __init__(self, result_path, model_name=\"NCF\"):\n",
    "        self.result_path = result_path\n",
    "        self.model_name = model_name\n",
    "        self.epoch_emissions = []\n",
    "        self.cumulative_emissions = []\n",
    "        self.epoch_rmse = []\n",
    "        self.epoch_mae = []\n",
    "        self.epoch_loss = []\n",
    "        self.total_emissions = 0.0\n",
    "        self.trackers = {}\n",
    "        \n",
    "        # Inicializar tracker principal\n",
    "        self.main_tracker = EmissionsTracker(\n",
    "            project_name=f\"{model_name}_total\",\n",
    "            output_dir=f\"{result_path}/emissions_reports\",\n",
    "            save_to_file=True,\n",
    "            log_level=\"error\",\n",
    "            save_to_api=False,\n",
    "            tracking_mode=\"process\"\n",
    "        )\n",
    "        try:\n",
    "            self.main_tracker.start()\n",
    "            print(\"Tracker principal iniciado correctamente\")\n",
    "        except Exception as e:\n",
    "            print(f\"Advertencia: No se pudo iniciar el tracker principal: {e}\")\n",
    "            self.main_tracker = None\n",
    "    \n",
    "    def start_epoch(self, epoch):\n",
    "        # Crear un tracker con un nombre único basado en timestamp\n",
    "        timestamp = int(time.time())\n",
    "        tracker_name = f\"{self.model_name}_epoch{epoch}_{timestamp}\"\n",
    "        \n",
    "        self.trackers[epoch] = EmissionsTracker(\n",
    "            project_name=tracker_name,\n",
    "            output_dir=f\"{self.result_path}/emissions_reports\",\n",
    "            save_to_file=True,\n",
    "            log_level=\"error\",\n",
    "            save_to_api=False,\n",
    "            tracking_mode=\"process\",\n",
    "            measure_power_secs=1,\n",
    "            allow_multiple_runs=True\n",
    "        )\n",
    "        try:\n",
    "            self.trackers[epoch].start()\n",
    "        except Exception as e:\n",
    "            print(f\"Advertencia: No se pudo iniciar el tracker para la época {epoch}: {e}\")\n",
    "            self.trackers[epoch] = None\n",
    "    \n",
    "    def end_epoch(self, epoch, loss, rmse=None, mae=None):\n",
    "        try:\n",
    "            epoch_co2 = 0.0\n",
    "            if epoch in self.trackers and self.trackers[epoch]:\n",
    "                try:\n",
    "                    epoch_co2 = self.trackers[epoch].stop() or 0.0\n",
    "                except Exception as e:\n",
    "                    print(f\"Advertencia: Error al detener el tracker para la época {epoch}: {e}\")\n",
    "                    epoch_co2 = 0.0\n",
    "            \n",
    "            # Acumular emisiones totales\n",
    "            self.total_emissions += epoch_co2\n",
    "            \n",
    "            # Guardar datos de esta época\n",
    "            self.epoch_emissions.append(epoch_co2)\n",
    "            self.cumulative_emissions.append(self.total_emissions)\n",
    "            self.epoch_loss.append(loss)\n",
    "            if rmse is not None:\n",
    "                self.epoch_rmse.append(rmse)\n",
    "            if mae is not None:\n",
    "                self.epoch_mae.append(mae)\n",
    "            \n",
    "            print(f\"Epoch {epoch} - Emisiones: {epoch_co2:.8f} kg, Acumulado: {self.total_emissions:.8f} kg, Loss: {loss:.4f}\")\n",
    "            if rmse is not None:\n",
    "                print(f\"RMSE: {rmse:.4f}\")\n",
    "            if mae is not None:\n",
    "                print(f\"MAE: {mae:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al medir emisiones en época {epoch}: {e}\")\n",
    "    \n",
    "    def end_training(self, final_rmse, final_mae=None):\n",
    "        try:\n",
    "            # Detener el tracker principal\n",
    "            final_emissions = 0.0\n",
    "            if hasattr(self, 'main_tracker') and self.main_tracker:\n",
    "                try:\n",
    "                    final_emissions = self.main_tracker.stop() or 0.0\n",
    "                    print(f\"\\nTotal CO2 Emissions: {final_emissions:.6f} kg\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error al detener el tracker principal: {e}\")\n",
    "                    final_emissions = self.total_emissions\n",
    "            else:\n",
    "                final_emissions = self.total_emissions\n",
    "            \n",
    "            # Asegurarse de que todos los trackers estén detenidos\n",
    "            for epoch, tracker in self.trackers.items():\n",
    "                if tracker is not None:\n",
    "                    try:\n",
    "                        tracker.stop()\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            # Si no hay datos de emisiones por época pero tenemos emisiones totales,\n",
    "            # crear al menos una entrada para gráficos\n",
    "            if not self.epoch_emissions and final_emissions > 0:\n",
    "                self.epoch_emissions = [final_emissions]\n",
    "                self.cumulative_emissions = [final_emissions]\n",
    "                if final_rmse is not None:\n",
    "                    self.epoch_rmse = [final_rmse]\n",
    "                if final_mae is not None:\n",
    "                    self.epoch_mae = [final_mae]\n",
    "            \n",
    "            # Si no hay datos, salir\n",
    "            if not self.epoch_emissions:\n",
    "                print(\"No hay datos de emisiones para graficar\")\n",
    "                return\n",
    "            \n",
    "            # Asegurarse de que tengamos un RMSE final si no se rastreó por época\n",
    "            if not self.epoch_rmse and final_rmse is not None:\n",
    "                self.epoch_rmse = [final_rmse] * len(self.epoch_emissions)\n",
    "            \n",
    "            # Crear dataframe con todos los datos\n",
    "            timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "            df = pd.DataFrame({\n",
    "                'epoch': range(len(self.epoch_emissions)),\n",
    "                'epoch_emissions_kg': self.epoch_emissions,\n",
    "                'cumulative_emissions_kg': self.cumulative_emissions,\n",
    "                'loss': self.epoch_loss if self.epoch_loss else [0.0] * len(self.epoch_emissions),\n",
    "                'rmse': self.epoch_rmse if self.epoch_rmse else [None] * len(self.epoch_emissions),\n",
    "                'mae': self.epoch_mae if self.epoch_mae else [None] * len(self.epoch_emissions)\n",
    "            })\n",
    "            \n",
    "            emissions_file = f'{self.result_path}/emissions_reports/emissions_metrics_{self.model_name}_{timestamp}.csv'\n",
    "            df.to_csv(emissions_file, index=False)\n",
    "            print(f\"Métricas de emisiones guardadas en: {emissions_file}\")\n",
    "            \n",
    "            # Graficar las relaciones\n",
    "            self.plot_emissions_vs_metrics(timestamp, final_rmse, final_mae)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error al generar gráficos de emisiones: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    def plot_emissions_vs_metrics(self, timestamp, final_rmse=None, final_mae=None):\n",
    "        \"\"\"Genera gráficos para emisiones vs métricas\"\"\"\n",
    "        \n",
    "        # Configurar estilo para fondo blanco y texto negro (más legible)\n",
    "        plt.style.use('default')\n",
    "        \n",
    "        # Usar RMSE por época si está disponible, sino crear lista con el RMSE final\n",
    "        if not self.epoch_rmse and final_rmse is not None:\n",
    "            self.epoch_rmse = [final_rmse] * len(self.epoch_emissions)\n",
    "        \n",
    "        try:\n",
    "            if self.epoch_rmse:\n",
    "                # 1. Emisiones acumulativas vs RMSE\n",
    "                plt.figure(figsize=(10, 6), facecolor='white')\n",
    "                plt.plot(self.cumulative_emissions, self.epoch_rmse, 'b-', marker='o')\n",
    "                \n",
    "                # Añadir etiquetas con el número de época\n",
    "                for i, (emissions, rmse) in enumerate(zip(self.cumulative_emissions, self.epoch_rmse)):\n",
    "                    plt.annotate(f\"{i}\", (emissions, rmse), textcoords=\"offset points\", \n",
    "                                xytext=(0,10), ha='center', fontsize=9, color='black')\n",
    "                    \n",
    "                plt.xlabel('Emisiones de CO2 acumuladas (kg)', color='black')\n",
    "                plt.ylabel('RMSE', color='black')\n",
    "                plt.title('Relación entre Emisiones Acumuladas y RMSE', color='black')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.tick_params(colors='black')\n",
    "                \n",
    "                file_path = f'{self.result_path}/emissions_plots/cumulative_emissions_vs_rmse_{self.model_name}_{timestamp}.png'\n",
    "                plt.savefig(file_path, facecolor='white')\n",
    "                plt.close()\n",
    "                print(f\"Gráfico guardado en: {file_path}\")\n",
    "            \n",
    "            # 2. Gráfico combinado: Emisiones por época y acumulativas\n",
    "            plt.figure(figsize=(12, 10), facecolor='white')\n",
    "            \n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.plot(range(len(self.epoch_emissions)), self.epoch_emissions, 'r-', marker='x')\n",
    "            plt.title('Emisiones por Época', color='black')\n",
    "            plt.xlabel('Época', color='black')\n",
    "            plt.ylabel('CO2 Emissions (kg)', color='black')\n",
    "            plt.tick_params(colors='black')\n",
    "            \n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.plot(range(len(self.cumulative_emissions)), self.cumulative_emissions, 'r-', marker='o')\n",
    "            plt.title('Emisiones Acumuladas por Época', color='black')\n",
    "            plt.xlabel('Época', color='black')\n",
    "            plt.ylabel('CO2 Emissions (kg)', color='black')\n",
    "            plt.tick_params(colors='black')\n",
    "            \n",
    "            if self.epoch_loss:\n",
    "                plt.subplot(2, 2, 3)\n",
    "                plt.plot(range(len(self.epoch_loss)), self.epoch_loss, 'g-', marker='o')\n",
    "                plt.title('Loss por Época', color='black')\n",
    "                plt.xlabel('Época', color='black')\n",
    "                plt.ylabel('Loss', color='black')\n",
    "                plt.tick_params(colors='black')\n",
    "            \n",
    "            if self.epoch_rmse:\n",
    "                plt.subplot(2, 2, 4)\n",
    "                plt.plot(range(len(self.epoch_rmse)), self.epoch_rmse, 'b-', marker='o')\n",
    "                plt.title('RMSE por Época', color='black')\n",
    "                plt.xlabel('Época', color='black')\n",
    "                plt.ylabel('RMSE', color='black')\n",
    "                plt.tick_params(colors='black')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            file_path = f'{self.result_path}/emissions_plots/metrics_by_epoch_{self.model_name}_{timestamp}.png'\n",
    "            plt.savefig(file_path, facecolor='white')\n",
    "            plt.close()\n",
    "            print(f\"Gráfico guardado en: {file_path}\")\n",
    "            \n",
    "            if self.epoch_rmse:\n",
    "                # 3. Scatter plot de rendimiento frente a emisiones acumulativas\n",
    "                plt.figure(figsize=(10, 6), facecolor='white')\n",
    "                \n",
    "                # Ajustar tamaño de los puntos según la época\n",
    "                sizes = [(i+1)*20 for i in range(len(self.cumulative_emissions))]\n",
    "                \n",
    "                scatter = plt.scatter(self.epoch_rmse, self.cumulative_emissions, \n",
    "                            color='blue', marker='o', s=sizes, alpha=0.7)\n",
    "                \n",
    "                # Añadir etiquetas de época\n",
    "                for i, (rmse, em) in enumerate(zip(self.epoch_rmse, self.cumulative_emissions)):\n",
    "                    plt.annotate(f\"{i}\", (rmse, em), textcoords=\"offset points\", \n",
    "                                xytext=(0,5), ha='center', fontsize=9, color='black')\n",
    "                \n",
    "                plt.ylabel('Emisiones de CO2 acumuladas (kg)', color='black')\n",
    "                plt.xlabel('RMSE', color='black')\n",
    "                plt.title('Relación entre RMSE y Emisiones Acumuladas', color='black')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.tick_params(colors='black')\n",
    "                \n",
    "                file_path = f'{self.result_path}/emissions_plots/cumulative_emissions_performance_scatter_{self.model_name}_{timestamp}.png'\n",
    "                plt.savefig(file_path, facecolor='white')\n",
    "                plt.close()\n",
    "                print(f\"Gráfico guardado en: {file_path}\")\n",
    "                \n",
    "            # 4. Si tenemos RMSE y MAE, crear un gráfico comparativo\n",
    "            if self.epoch_mae and self.epoch_rmse:\n",
    "                plt.figure(figsize=(10, 6), facecolor='white')\n",
    "                plt.plot(range(len(self.epoch_rmse)), self.epoch_rmse, 'b-', marker='o', label='RMSE')\n",
    "                plt.plot(range(len(self.epoch_mae)), self.epoch_mae, 'g-', marker='s', label='MAE')\n",
    "                plt.title('Comparación de RMSE y MAE por Época', color='black')\n",
    "                plt.xlabel('Época', color='black')\n",
    "                plt.ylabel('Error', color='black')\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.tick_params(colors='black')\n",
    "                \n",
    "                file_path = f'{self.result_path}/emissions_plots/rmse_vs_mae_{self.model_name}_{timestamp}.png'\n",
    "                plt.savefig(file_path, facecolor='white')\n",
    "                plt.close()\n",
    "                print(f\"Gráfico comparativo guardado en: {file_path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error al generar los gráficos: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94912930",
   "metadata": {},
   "source": [
    "# **Cargar el dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31881d0b-a0a1-4c44-917d-ae2913acc374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movieId                               title                        genres\n",
      "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
      "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
      "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
      "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
      "4        5  Father of the Bride Part II (1995)                        Comedy\n",
      "   userId  movieId  rating  timestamp\n",
      "0       1     1193       5  978300760\n",
      "1       1      661       3  978302109\n",
      "2       1      914       3  978301968\n",
      "3       1     3408       4  978300275\n",
      "4       1     2355       5  978824291\n"
     ]
    }
   ],
   "source": [
    "# Load MovieLens dataset\n",
    "movies_df = pd.read_csv('C://Users//xpati//Documents//TFG//ml-1m//movies.dat', \n",
    "                       sep='::', \n",
    "                       header=None, \n",
    "                       names=['movieId', 'title', 'genres'],\n",
    "                       encoding='latin-1',\n",
    "                       engine='python')\n",
    "\n",
    "ratings_df = pd.read_csv('C://Users//xpati//Documents//TFG//ml-1m//ratings.dat', \n",
    "                        sep='::', \n",
    "                        header=None, \n",
    "                        names=['userId', 'movieId', 'rating', 'timestamp'],\n",
    "                        encoding='latin-1',\n",
    "                        engine='python')\n",
    "\n",
    "# Display first few rows\n",
    "print(movies_df.head())\n",
    "print(ratings_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a355e275-95bc-4197-93f8-ae0cee50e4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 6040\n",
      "Number of movies: 3706\n"
     ]
    }
   ],
   "source": [
    "# Label encoding of user and movie IDs\n",
    "user_encoder = LabelEncoder()\n",
    "movie_encoder = LabelEncoder()\n",
    "\n",
    "ratings_df['userId'] = user_encoder.fit_transform(ratings_df['userId'])\n",
    "ratings_df['movieId'] = movie_encoder.fit_transform(ratings_df['movieId'])\n",
    "\n",
    "# Obtener número de usuarios y películas únicas\n",
    "n_users = len(user_encoder.classes_)\n",
    "n_movies = len(movie_encoder.classes_)\n",
    "\n",
    "print(f'Number of users: {n_users}')\n",
    "print(f'Number of movies: {n_movies}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88982cc0-2351-4186-952f-6da11723040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, ratings_df):\n",
    "        self.users = ratings_df['userId'].values\n",
    "        self.movies = ratings_df['movieId'].values\n",
    "        self.ratings = ratings_df['rating'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.users[idx], dtype=torch.long), \\\n",
    "               torch.tensor(self.movies[idx], dtype=torch.long), \\\n",
    "               torch.tensor(self.ratings[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ac6dfd",
   "metadata": {},
   "source": [
    "# ***NCF (Neural Collaborative Filtering) Recommender***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6ed84d0-1de4-4ebb-9fdb-21a15db3acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCollaborativeFiltering(nn.Module):\n",
    "    def __init__(self, n_users, n_movies, embedding_dim=50, hidden_dim=128):\n",
    "        super(NeuralCollaborativeFiltering, self).__init__()\n",
    "        \n",
    "        # Embedding layers for users and movies\n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.movie_embedding = nn.Embedding(n_movies, embedding_dim)\n",
    "        \n",
    "        # MLP layers for neural network\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, user, movie):\n",
    "        # Get embeddings\n",
    "        user_embedded = self.user_embedding(user)\n",
    "        movie_embedded = self.movie_embedding(movie)\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        x = torch.cat([user_embedded, movie_embedded], dim=1)\n",
    "        \n",
    "        # Pass through MLP\n",
    "        return self.mlp(x).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1f88e6",
   "metadata": {},
   "source": [
    "# ***NCF con historial***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79f98ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCFWithHistory(nn.Module):\n",
    "    \"\"\"Neural Collaborative Filtering con recomendaciones basadas en historial\"\"\"\n",
    "    def __init__(self, n_users, n_movies, embedding_dim=50, hidden_dim=128, history_weight=0.3):\n",
    "        super(NCFWithHistory, self).__init__()\n",
    "        \n",
    "        # Parámetros básicos\n",
    "        self.n_users = n_users\n",
    "        self.n_movies = n_movies\n",
    "        self.history_weight = history_weight\n",
    "        \n",
    "        # Capas de embedding para usuarios y películas\n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.movie_embedding = nn.Embedding(n_movies, embedding_dim)\n",
    "        \n",
    "        # Capas MLP para la red neuronal\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Historial de usuario y matriz de similitud\n",
    "        self.user_histories = {}\n",
    "        self.item_similarities = None\n",
    "    \n",
    "    def forward(self, user, movie):\n",
    "        \"\"\"Forward pass a través del modelo\"\"\"\n",
    "        # Obtener embeddings\n",
    "        user_embedded = self.user_embedding(user)\n",
    "        movie_embedded = self.movie_embedding(movie)\n",
    "        \n",
    "        # Concatenar embeddings\n",
    "        x = torch.cat([user_embedded, movie_embedded], dim=1)\n",
    "        \n",
    "        # Pasar por la red MLP\n",
    "        return self.mlp(x).squeeze()\n",
    "    \n",
    "    def set_user_histories(self, ratings_df):\n",
    "        \"\"\"Configurar historiales de usuario a partir de datos de entrenamiento\"\"\"\n",
    "        print(\"Construyendo historiales de usuario...\")\n",
    "        self.user_histories = {}\n",
    "        \n",
    "        # Agrupar valoraciones por usuario\n",
    "        for user_id in range(self.n_users):\n",
    "            # Filtrar valoraciones para este usuario\n",
    "            user_data = ratings_df[ratings_df['userId'] == user_id]\n",
    "            \n",
    "            if len(user_data) > 0:\n",
    "                # Crear historial como un diccionario {item_id: rating}\n",
    "                user_history = dict(zip(user_data['movieId'].values, user_data['rating'].values))\n",
    "                self.user_histories[user_id] = user_history\n",
    "        \n",
    "        print(f\"Construido historial para {len(self.user_histories)} usuarios\")\n",
    "    \n",
    "    def compute_item_similarities(self, ratings_df):\n",
    "        \"\"\"Calcular matriz de similitud entre ítems basada en valoraciones de usuario\"\"\"\n",
    "        print(\"Calculando similitudes entre películas...\")\n",
    "        \n",
    "        # Inicializar matriz de similitud\n",
    "        self.item_similarities = np.zeros((self.n_movies, self.n_movies))\n",
    "        \n",
    "        # Crear matriz de valoraciones usuarios-películas\n",
    "        rating_matrix = np.zeros((self.n_users, self.n_movies))\n",
    "        for index, row in ratings_df.iterrows():\n",
    "            rating_matrix[row['userId'], row['movieId']] = row['rating']\n",
    "        \n",
    "        # Obtener perfiles de película (transponer matriz para obtener perfiles por película)\n",
    "        item_profiles = rating_matrix.T\n",
    "        \n",
    "        # Calcular normas para todas las películas\n",
    "        item_norms = np.array([np.linalg.norm(item_profiles[i]) for i in range(self.n_movies)])\n",
    "        \n",
    "        # Solo procesar películas con normas no cero\n",
    "        valid_items = np.where(item_norms > 0)[0]\n",
    "        print(f\"Calculando similitudes para {len(valid_items)} películas válidas...\")\n",
    "        \n",
    "        # Procesar por lotes para mostrar progreso\n",
    "        batch_size = 100\n",
    "        total_batches = (len(valid_items) + batch_size - 1) // batch_size\n",
    "        \n",
    "        for batch_idx in range(total_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = min((batch_idx + 1) * batch_size, len(valid_items))\n",
    "            batch_items = valid_items[batch_start:batch_end]\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"  Procesando lote {batch_idx+1}/{total_batches}...\")\n",
    "            \n",
    "            for i_idx in range(len(batch_items)):\n",
    "                i = batch_items[i_idx]\n",
    "                i_profile = item_profiles[i]\n",
    "                i_norm = item_norms[i]\n",
    "                \n",
    "                # Calcular similitudes con otras películas\n",
    "                for j_idx in range(i_idx, len(valid_items)):\n",
    "                    j = valid_items[j_idx]\n",
    "                    if j < i:\n",
    "                        continue  # Omitir si ya calculamos este par\n",
    "                    \n",
    "                    j_profile = item_profiles[j]\n",
    "                    j_norm = item_norms[j]\n",
    "                    \n",
    "                    # Calcular similitud de coseno\n",
    "                    if i_norm * j_norm > 0:\n",
    "                        sim = np.dot(i_profile, j_profile) / (i_norm * j_norm)\n",
    "                    else:\n",
    "                        sim = 0.0\n",
    "                    \n",
    "                    # Actualizar matriz de similitud (simétrica)\n",
    "                    self.item_similarities[i, j] = sim\n",
    "                    if i != j:  # Evitar establecer valores diagonales dos veces\n",
    "                        self.item_similarities[j, i] = sim\n",
    "        \n",
    "        print(\"Similitudes calculadas\")\n",
    "    \n",
    "    def predict_with_history(self, user_id, item_id, ncf_prediction):\n",
    "        \"\"\"\n",
    "        Ajustar predicción de NCF usando historial de usuario\n",
    "        Retorna el promedio ponderado de la predicción NCF y la predicción basada en historial\n",
    "        \"\"\"\n",
    "        # Si no hay historial o datos de similitud, retornar predicción NCF\n",
    "        if user_id not in self.user_histories or self.item_similarities is None:\n",
    "            return ncf_prediction\n",
    "        \n",
    "        user_history = self.user_histories[user_id]\n",
    "        \n",
    "        # Si el usuario no tiene historial, retornar predicción NCF\n",
    "        if not user_history:\n",
    "            return ncf_prediction\n",
    "        \n",
    "        # Obtener similitudes entre película objetivo y películas calificadas\n",
    "        similar_items = []\n",
    "        \n",
    "        for hist_item_id, hist_rating in user_history.items():\n",
    "            similarity = self.item_similarities[item_id, hist_item_id]\n",
    "            \n",
    "            # Solo considerar películas algo similares (umbral arbitrario y ajustable)\n",
    "            if similarity > 0.1:\n",
    "                similar_items.append((hist_item_id, hist_rating, similarity))\n",
    "        \n",
    "        # Si no se encontraron películas similares, retornar predicción NCF\n",
    "        if not similar_items:\n",
    "            return ncf_prediction\n",
    "        \n",
    "        # Calcular promedio ponderado de valoraciones de películas similares\n",
    "        total_sim = sum(sim for _, _, sim in similar_items)\n",
    "        history_prediction = sum(rating * sim for _, rating, sim in similar_items) / total_sim\n",
    "        \n",
    "        # Combinar predicciones\n",
    "        final_prediction = (1 - self.history_weight) * ncf_prediction + self.history_weight * history_prediction\n",
    "        \n",
    "        return final_prediction\n",
    "    \n",
    "    def predict_for_user(self, user_id, movie_id):\n",
    "        \"\"\"Predecir valoración para un usuario y película usando NCF y historial\"\"\"\n",
    "        # Convertir a tensores\n",
    "        user_tensor = torch.tensor([user_id], dtype=torch.long)\n",
    "        movie_tensor = torch.tensor([movie_id], dtype=torch.long)\n",
    "        \n",
    "        # Obtener predicción NCF\n",
    "        with torch.no_grad():\n",
    "            ncf_prediction = self.forward(user_tensor, movie_tensor).item()\n",
    "        \n",
    "        # Ajustar con historial\n",
    "        adjusted_prediction = self.predict_with_history(user_id, movie_id, ncf_prediction)\n",
    "        \n",
    "        return ncf_prediction, adjusted_prediction\n",
    "    \n",
    "    def demonstrate_prediction(self, user_id, item_id):\n",
    "        \"\"\"Demostrar proceso de predicción para una película\"\"\"\n",
    "        # Obtener predicción NCF\n",
    "        user_tensor = torch.tensor([user_id], dtype=torch.long)\n",
    "        movie_tensor = torch.tensor([item_id], dtype=torch.long)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ncf_prediction = self.forward(user_tensor, movie_tensor).item()\n",
    "        \n",
    "        # Obtener predicción ajustada\n",
    "        adjusted_prediction = self.predict_with_history(user_id, item_id, ncf_prediction)\n",
    "        \n",
    "        # Imprimir información sobre la predicción\n",
    "        print(f\"\\nDemostrando predicción para Usuario {user_id}, Película {item_id}:\")\n",
    "        print(f\"Predicción NCF: {ncf_prediction:.4f}\")\n",
    "        \n",
    "        if user_id in self.user_histories:\n",
    "            history = self.user_histories[user_id]\n",
    "            print(f\"Usuario ha calificado {len(history)} películas\")\n",
    "            \n",
    "            # Encontrar películas similares\n",
    "            similar_items = []\n",
    "            for hist_item_id, hist_rating in history.items():\n",
    "                if self.item_similarities is not None:\n",
    "                    similarity = self.item_similarities[item_id, hist_item_id]\n",
    "                    if similarity > 0.1:\n",
    "                        similar_items.append((hist_item_id, hist_rating, similarity))\n",
    "            \n",
    "            if similar_items:\n",
    "                print(\"\\nPelículas similares encontradas en historial:\")\n",
    "                print(f\"{'ID Película':<10}{'Valoración':<10}{'Similitud':<15}\")\n",
    "                \n",
    "                for hist_item, hist_rating, sim in sorted(similar_items, key=lambda x: x[2], reverse=True)[:5]:\n",
    "                    print(f\"{hist_item:<10}{hist_rating:<10.2f}{sim:<15.4f}\")\n",
    "                \n",
    "                total_sim = sum(sim for _, _, sim in similar_items)\n",
    "                history_prediction = sum(rating * sim for _, rating, sim in similar_items) / total_sim\n",
    "                print(f\"\\nPredicción basada en historial: {history_prediction:.4f}\")\n",
    "                print(f\"Predicción final ponderada ({self.history_weight} peso historial): {adjusted_prediction:.4f}\")\n",
    "            else:\n",
    "                print(\"No se encontraron películas similares en historial de usuario\")\n",
    "        else:\n",
    "            print(\"Usuario no tiene historial de valoraciones\")\n",
    "        \n",
    "        return adjusted_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c3172b",
   "metadata": {},
   "source": [
    "# **Entrenamiento y Evaluación**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9abb3717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparando los datasets...\n",
      "Datos preparados: 800167 muestras de entrenamiento, 200042 muestras de validación\n",
      "Inicializando trackers...\n",
      "Tracker principal iniciado correctamente\n",
      "\n",
      "Comenzando entrenamiento (50 épocas)...\n",
      "\n",
      "Epoch 0 Metrics:\n",
      "  Time: 47.11s\n",
      "  Memory: 594.14MB\n",
      "  CPU: 46.3%\n",
      "  Loss: 1.3755\n",
      "  RMSE: 0.9598\n",
      "  MAE: 0.7659\n",
      "Epoch 0 - Emisiones: 0.00002595 kg, Acumulado: 0.00002595 kg, Loss: 1.3755\n",
      "RMSE: 0.9598\n",
      "MAE: 0.7659\n",
      "Epoch 1/50, Training Loss: 1.3755, Validation RMSE: 0.9598, Validation MAE: 0.7659\n",
      "\n",
      "Epoch 1 Metrics:\n",
      "  Time: 43.01s\n",
      "  Memory: 633.51MB\n",
      "  CPU: 70.5%\n",
      "  Loss: 0.8670\n",
      "  RMSE: 0.9346\n",
      "  MAE: 0.7432\n",
      "Epoch 1 - Emisiones: 0.00002331 kg, Acumulado: 0.00004926 kg, Loss: 0.8670\n",
      "RMSE: 0.9346\n",
      "MAE: 0.7432\n",
      "Epoch 2/50, Training Loss: 0.8670, Validation RMSE: 0.9346, Validation MAE: 0.7432\n",
      "\n",
      "Epoch 2 Metrics:\n",
      "  Time: 34.29s\n",
      "  Memory: 635.03MB\n",
      "  CPU: 72.1%\n",
      "  Loss: 0.8276\n",
      "  RMSE: 0.9297\n",
      "  MAE: 0.7353\n",
      "Epoch 2 - Emisiones: 0.00001818 kg, Acumulado: 0.00006744 kg, Loss: 0.8276\n",
      "RMSE: 0.9297\n",
      "MAE: 0.7353\n",
      "Epoch 3/50, Training Loss: 0.8276, Validation RMSE: 0.9297, Validation MAE: 0.7353\n",
      "\n",
      "Epoch 3 Metrics:\n",
      "  Time: 34.85s\n",
      "  Memory: 636.88MB\n",
      "  CPU: 54.9%\n",
      "  Loss: 0.8087\n",
      "  RMSE: 0.9277\n",
      "  MAE: 0.7357\n",
      "Epoch 3 - Emisiones: 0.00001848 kg, Acumulado: 0.00008591 kg, Loss: 0.8087\n",
      "RMSE: 0.9277\n",
      "MAE: 0.7357\n",
      "Epoch 4/50, Training Loss: 0.8087, Validation RMSE: 0.9277, Validation MAE: 0.7357\n",
      "\n",
      "Epoch 4 Metrics:\n",
      "  Time: 35.14s\n",
      "  Memory: 638.32MB\n",
      "  CPU: 54.8%\n",
      "  Loss: 0.7948\n",
      "  RMSE: 0.9278\n",
      "  MAE: 0.7345\n",
      "Epoch 4 - Emisiones: 0.00001859 kg, Acumulado: 0.00010450 kg, Loss: 0.7948\n",
      "RMSE: 0.9278\n",
      "MAE: 0.7345\n",
      "Epoch 5/50, Training Loss: 0.7948, Validation RMSE: 0.9278, Validation MAE: 0.7345\n",
      "\n",
      "Epoch 5 Metrics:\n",
      "  Time: 34.34s\n",
      "  Memory: 638.82MB\n",
      "  CPU: 59.6%\n",
      "  Loss: 0.7838\n",
      "  RMSE: 0.9263\n",
      "  MAE: 0.7305\n",
      "Epoch 5 - Emisiones: 0.00001819 kg, Acumulado: 0.00012269 kg, Loss: 0.7838\n",
      "RMSE: 0.9263\n",
      "MAE: 0.7305\n",
      "Epoch 6/50, Training Loss: 0.7838, Validation RMSE: 0.9263, Validation MAE: 0.7305\n",
      "\n",
      "Epoch 6 Metrics:\n",
      "  Time: 34.77s\n",
      "  Memory: 639.31MB\n",
      "  CPU: 54.5%\n",
      "  Loss: 0.7728\n",
      "  RMSE: 0.9256\n",
      "  MAE: 0.7333\n",
      "Epoch 6 - Emisiones: 0.00001846 kg, Acumulado: 0.00014116 kg, Loss: 0.7728\n",
      "RMSE: 0.9256\n",
      "MAE: 0.7333\n",
      "Epoch 7/50, Training Loss: 0.7728, Validation RMSE: 0.9256, Validation MAE: 0.7333\n",
      "\n",
      "Epoch 7 Metrics:\n",
      "  Time: 34.19s\n",
      "  Memory: 639.82MB\n",
      "  CPU: 54.4%\n",
      "  Loss: 0.7618\n",
      "  RMSE: 0.9268\n",
      "  MAE: 0.7330\n",
      "Epoch 7 - Emisiones: 0.00001812 kg, Acumulado: 0.00015927 kg, Loss: 0.7618\n",
      "RMSE: 0.9268\n",
      "MAE: 0.7330\n",
      "Epoch 8/50, Training Loss: 0.7618, Validation RMSE: 0.9268, Validation MAE: 0.7330\n",
      "\n",
      "Epoch 8 Metrics:\n",
      "  Time: 34.51s\n",
      "  Memory: 640.12MB\n",
      "  CPU: 54.6%\n",
      "  Loss: 0.7504\n",
      "  RMSE: 0.9265\n",
      "  MAE: 0.7338\n",
      "Epoch 8 - Emisiones: 0.00001818 kg, Acumulado: 0.00017745 kg, Loss: 0.7504\n",
      "RMSE: 0.9265\n",
      "MAE: 0.7338\n",
      "Epoch 9/50, Training Loss: 0.7504, Validation RMSE: 0.9265, Validation MAE: 0.7338\n",
      "\n",
      "Epoch 9 Metrics:\n",
      "  Time: 34.93s\n",
      "  Memory: 640.64MB\n",
      "  CPU: 57.0%\n",
      "  Loss: 0.7385\n",
      "  RMSE: 0.9282\n",
      "  MAE: 0.7362\n",
      "Epoch 9 - Emisiones: 0.00001856 kg, Acumulado: 0.00019601 kg, Loss: 0.7385\n",
      "RMSE: 0.9282\n",
      "MAE: 0.7362\n",
      "Epoch 10/50, Training Loss: 0.7385, Validation RMSE: 0.9282, Validation MAE: 0.7362\n",
      "\n",
      "Epoch 10 Metrics:\n",
      "  Time: 37.08s\n",
      "  Memory: 641.14MB\n",
      "  CPU: 56.1%\n",
      "  Loss: 0.7263\n",
      "  RMSE: 0.9302\n",
      "  MAE: 0.7331\n",
      "Epoch 10 - Emisiones: 0.00001991 kg, Acumulado: 0.00021592 kg, Loss: 0.7263\n",
      "RMSE: 0.9302\n",
      "MAE: 0.7331\n",
      "Epoch 11/50, Training Loss: 0.7263, Validation RMSE: 0.9302, Validation MAE: 0.7331\n",
      "\n",
      "Epoch 11 Metrics:\n",
      "  Time: 33.68s\n",
      "  Memory: 641.70MB\n",
      "  CPU: 62.1%\n",
      "  Loss: 0.7137\n",
      "  RMSE: 0.9344\n",
      "  MAE: 0.7337\n",
      "Epoch 11 - Emisiones: 0.00001777 kg, Acumulado: 0.00023369 kg, Loss: 0.7137\n",
      "RMSE: 0.9344\n",
      "MAE: 0.7337\n",
      "Epoch 12/50, Training Loss: 0.7137, Validation RMSE: 0.9344, Validation MAE: 0.7337\n",
      "\n",
      "Epoch 12 Metrics:\n",
      "  Time: 36.14s\n",
      "  Memory: 641.95MB\n",
      "  CPU: 54.0%\n",
      "  Loss: 0.7009\n",
      "  RMSE: 0.9338\n",
      "  MAE: 0.7370\n",
      "Epoch 12 - Emisiones: 0.00001933 kg, Acumulado: 0.00025302 kg, Loss: 0.7009\n",
      "RMSE: 0.9338\n",
      "MAE: 0.7370\n",
      "Epoch 13/50, Training Loss: 0.7009, Validation RMSE: 0.9338, Validation MAE: 0.7370\n",
      "\n",
      "Epoch 13 Metrics:\n",
      "  Time: 36.31s\n",
      "  Memory: 642.46MB\n",
      "  CPU: 59.3%\n",
      "  Loss: 0.6874\n",
      "  RMSE: 0.9355\n",
      "  MAE: 0.7392\n",
      "Epoch 13 - Emisiones: 0.00001942 kg, Acumulado: 0.00027243 kg, Loss: 0.6874\n",
      "RMSE: 0.9355\n",
      "MAE: 0.7392\n",
      "Epoch 14/50, Training Loss: 0.6874, Validation RMSE: 0.9355, Validation MAE: 0.7392\n",
      "\n",
      "Epoch 14 Metrics:\n",
      "  Time: 35.49s\n",
      "  Memory: 642.97MB\n",
      "  CPU: 62.1%\n",
      "  Loss: 0.6739\n",
      "  RMSE: 0.9374\n",
      "  MAE: 0.7399\n",
      "Epoch 14 - Emisiones: 0.00001888 kg, Acumulado: 0.00029131 kg, Loss: 0.6739\n",
      "RMSE: 0.9374\n",
      "MAE: 0.7399\n",
      "Epoch 15/50, Training Loss: 0.6739, Validation RMSE: 0.9374, Validation MAE: 0.7399\n",
      "\n",
      "Epoch 15 Metrics:\n",
      "  Time: 36.84s\n",
      "  Memory: 643.42MB\n",
      "  CPU: 60.2%\n",
      "  Loss: 0.6609\n",
      "  RMSE: 0.9397\n",
      "  MAE: 0.7422\n",
      "Epoch 15 - Emisiones: 0.00001967 kg, Acumulado: 0.00031098 kg, Loss: 0.6609\n",
      "RMSE: 0.9397\n",
      "MAE: 0.7422\n",
      "Epoch 16/50, Training Loss: 0.6609, Validation RMSE: 0.9397, Validation MAE: 0.7422\n",
      "\n",
      "Epoch 16 Metrics:\n",
      "  Time: 45.69s\n",
      "  Memory: 643.66MB\n",
      "  CPU: 58.2%\n",
      "  Loss: 0.6475\n",
      "  RMSE: 0.9423\n",
      "  MAE: 0.7465\n",
      "Epoch 16 - Emisiones: 0.00002515 kg, Acumulado: 0.00033613 kg, Loss: 0.6475\n",
      "RMSE: 0.9423\n",
      "MAE: 0.7465\n",
      "Epoch 17/50, Training Loss: 0.6475, Validation RMSE: 0.9423, Validation MAE: 0.7465\n",
      "\n",
      "Epoch 17 Metrics:\n",
      "  Time: 37.83s\n",
      "  Memory: 643.92MB\n",
      "  CPU: 75.0%\n",
      "  Loss: 0.6344\n",
      "  RMSE: 0.9460\n",
      "  MAE: 0.7468\n",
      "Epoch 17 - Emisiones: 0.00002018 kg, Acumulado: 0.00035632 kg, Loss: 0.6344\n",
      "RMSE: 0.9460\n",
      "MAE: 0.7468\n",
      "Epoch 18/50, Training Loss: 0.6344, Validation RMSE: 0.9460, Validation MAE: 0.7468\n",
      "\n",
      "Epoch 18 Metrics:\n",
      "  Time: 32.66s\n",
      "  Memory: 644.17MB\n",
      "  CPU: 65.5%\n",
      "  Loss: 0.6215\n",
      "  RMSE: 0.9535\n",
      "  MAE: 0.7485\n",
      "Epoch 18 - Emisiones: 0.00001718 kg, Acumulado: 0.00037349 kg, Loss: 0.6215\n",
      "RMSE: 0.9535\n",
      "MAE: 0.7485\n",
      "Epoch 19/50, Training Loss: 0.6215, Validation RMSE: 0.9535, Validation MAE: 0.7485\n",
      "\n",
      "Epoch 19 Metrics:\n",
      "  Time: 32.91s\n",
      "  Memory: 644.68MB\n",
      "  CPU: 51.3%\n",
      "  Loss: 0.6085\n",
      "  RMSE: 0.9530\n",
      "  MAE: 0.7523\n",
      "Epoch 19 - Emisiones: 0.00001732 kg, Acumulado: 0.00039082 kg, Loss: 0.6085\n",
      "RMSE: 0.9530\n",
      "MAE: 0.7523\n",
      "Epoch 20/50, Training Loss: 0.6085, Validation RMSE: 0.9530, Validation MAE: 0.7523\n",
      "\n",
      "Epoch 20 Metrics:\n",
      "  Time: 33.58s\n",
      "  Memory: 645.17MB\n",
      "  CPU: 51.5%\n",
      "  Loss: 0.5961\n",
      "  RMSE: 0.9548\n",
      "  MAE: 0.7547\n",
      "Epoch 20 - Emisiones: 0.00001773 kg, Acumulado: 0.00040855 kg, Loss: 0.5961\n",
      "RMSE: 0.9548\n",
      "MAE: 0.7547\n",
      "Epoch 21/50, Training Loss: 0.5961, Validation RMSE: 0.9548, Validation MAE: 0.7547\n",
      "\n",
      "Epoch 21 Metrics:\n",
      "  Time: 32.57s\n",
      "  Memory: 645.85MB\n",
      "  CPU: 52.8%\n",
      "  Loss: 0.5837\n",
      "  RMSE: 0.9585\n",
      "  MAE: 0.7556\n",
      "Epoch 21 - Emisiones: 0.00001713 kg, Acumulado: 0.00042567 kg, Loss: 0.5837\n",
      "RMSE: 0.9585\n",
      "MAE: 0.7556\n",
      "Epoch 22/50, Training Loss: 0.5837, Validation RMSE: 0.9585, Validation MAE: 0.7556\n",
      "\n",
      "Epoch 22 Metrics:\n",
      "  Time: 33.04s\n",
      "  Memory: 646.45MB\n",
      "  CPU: 50.2%\n",
      "  Loss: 0.5720\n",
      "  RMSE: 0.9633\n",
      "  MAE: 0.7578\n",
      "Epoch 22 - Emisiones: 0.00001742 kg, Acumulado: 0.00044310 kg, Loss: 0.5720\n",
      "RMSE: 0.9633\n",
      "MAE: 0.7578\n",
      "Epoch 23/50, Training Loss: 0.5720, Validation RMSE: 0.9633, Validation MAE: 0.7578\n",
      "\n",
      "Epoch 23 Metrics:\n",
      "  Time: 32.99s\n",
      "  Memory: 646.70MB\n",
      "  CPU: 52.6%\n",
      "  Loss: 0.5606\n",
      "  RMSE: 0.9657\n",
      "  MAE: 0.7635\n",
      "Epoch 23 - Emisiones: 0.00001735 kg, Acumulado: 0.00046045 kg, Loss: 0.5606\n",
      "RMSE: 0.9657\n",
      "MAE: 0.7635\n",
      "Epoch 24/50, Training Loss: 0.5606, Validation RMSE: 0.9657, Validation MAE: 0.7635\n",
      "\n",
      "Epoch 24 Metrics:\n",
      "  Time: 33.43s\n",
      "  Memory: 646.95MB\n",
      "  CPU: 52.2%\n",
      "  Loss: 0.5491\n",
      "  RMSE: 0.9693\n",
      "  MAE: 0.7667\n",
      "Epoch 24 - Emisiones: 0.00001766 kg, Acumulado: 0.00047811 kg, Loss: 0.5491\n",
      "RMSE: 0.9693\n",
      "MAE: 0.7667\n",
      "Epoch 25/50, Training Loss: 0.5491, Validation RMSE: 0.9693, Validation MAE: 0.7667\n",
      "\n",
      "Epoch 25 Metrics:\n",
      "  Time: 33.41s\n",
      "  Memory: 647.45MB\n",
      "  CPU: 52.2%\n",
      "  Loss: 0.5378\n",
      "  RMSE: 0.9739\n",
      "  MAE: 0.7692\n",
      "Epoch 25 - Emisiones: 0.00001765 kg, Acumulado: 0.00049576 kg, Loss: 0.5378\n",
      "RMSE: 0.9739\n",
      "MAE: 0.7692\n",
      "Epoch 26/50, Training Loss: 0.5378, Validation RMSE: 0.9739, Validation MAE: 0.7692\n",
      "\n",
      "Epoch 26 Metrics:\n",
      "  Time: 33.19s\n",
      "  Memory: 633.83MB\n",
      "  CPU: 53.8%\n",
      "  Loss: 0.5277\n",
      "  RMSE: 0.9819\n",
      "  MAE: 0.7706\n",
      "Epoch 26 - Emisiones: 0.00001748 kg, Acumulado: 0.00051324 kg, Loss: 0.5277\n",
      "RMSE: 0.9819\n",
      "MAE: 0.7706\n",
      "Epoch 27/50, Training Loss: 0.5277, Validation RMSE: 0.9819, Validation MAE: 0.7706\n",
      "\n",
      "Epoch 27 Metrics:\n",
      "  Time: 31.56s\n",
      "  Memory: 635.16MB\n",
      "  CPU: 51.8%\n",
      "  Loss: 0.5175\n",
      "  RMSE: 0.9817\n",
      "  MAE: 0.7767\n",
      "Epoch 27 - Emisiones: 0.00001650 kg, Acumulado: 0.00052973 kg, Loss: 0.5175\n",
      "RMSE: 0.9817\n",
      "MAE: 0.7767\n",
      "Epoch 28/50, Training Loss: 0.5175, Validation RMSE: 0.9817, Validation MAE: 0.7767\n",
      "\n",
      "Epoch 28 Metrics:\n",
      "  Time: 31.08s\n",
      "  Memory: 635.57MB\n",
      "  CPU: 49.9%\n",
      "  Loss: 0.5077\n",
      "  RMSE: 0.9830\n",
      "  MAE: 0.7775\n",
      "Epoch 28 - Emisiones: 0.00001619 kg, Acumulado: 0.00054592 kg, Loss: 0.5077\n",
      "RMSE: 0.9830\n",
      "MAE: 0.7775\n",
      "Epoch 29/50, Training Loss: 0.5077, Validation RMSE: 0.9830, Validation MAE: 0.7775\n",
      "\n",
      "Epoch 29 Metrics:\n",
      "  Time: 31.14s\n",
      "  Memory: 635.82MB\n",
      "  CPU: 49.0%\n",
      "  Loss: 0.4978\n",
      "  RMSE: 0.9861\n",
      "  MAE: 0.7773\n",
      "Epoch 29 - Emisiones: 0.00001624 kg, Acumulado: 0.00056216 kg, Loss: 0.4978\n",
      "RMSE: 0.9861\n",
      "MAE: 0.7773\n",
      "Epoch 30/50, Training Loss: 0.4978, Validation RMSE: 0.9861, Validation MAE: 0.7773\n",
      "\n",
      "Epoch 30 Metrics:\n",
      "  Time: 30.94s\n",
      "  Memory: 636.15MB\n",
      "  CPU: 49.1%\n",
      "  Loss: 0.4889\n",
      "  RMSE: 0.9897\n",
      "  MAE: 0.7814\n",
      "Epoch 30 - Emisiones: 0.00001611 kg, Acumulado: 0.00057826 kg, Loss: 0.4889\n",
      "RMSE: 0.9897\n",
      "MAE: 0.7814\n",
      "Epoch 31/50, Training Loss: 0.4889, Validation RMSE: 0.9897, Validation MAE: 0.7814\n",
      "\n",
      "Epoch 31 Metrics:\n",
      "  Time: 32.01s\n",
      "  Memory: 636.40MB\n",
      "  CPU: 48.7%\n",
      "  Loss: 0.4802\n",
      "  RMSE: 0.9940\n",
      "  MAE: 0.7821\n",
      "Epoch 31 - Emisiones: 0.00001680 kg, Acumulado: 0.00059507 kg, Loss: 0.4802\n",
      "RMSE: 0.9940\n",
      "MAE: 0.7821\n",
      "Epoch 32/50, Training Loss: 0.4802, Validation RMSE: 0.9940, Validation MAE: 0.7821\n",
      "\n",
      "Epoch 32 Metrics:\n",
      "  Time: 33.20s\n",
      "  Memory: 636.65MB\n",
      "  CPU: 51.8%\n",
      "  Loss: 0.4714\n",
      "  RMSE: 0.9990\n",
      "  MAE: 0.7860\n",
      "Epoch 32 - Emisiones: 0.00001753 kg, Acumulado: 0.00061260 kg, Loss: 0.4714\n",
      "RMSE: 0.9990\n",
      "MAE: 0.7860\n",
      "Epoch 33/50, Training Loss: 0.4714, Validation RMSE: 0.9990, Validation MAE: 0.7860\n",
      "\n",
      "Epoch 33 Metrics:\n",
      "  Time: 32.73s\n",
      "  Memory: 636.91MB\n",
      "  CPU: 51.5%\n",
      "  Loss: 0.4631\n",
      "  RMSE: 1.0027\n",
      "  MAE: 0.7919\n",
      "Epoch 33 - Emisiones: 0.00001722 kg, Acumulado: 0.00062982 kg, Loss: 0.4631\n",
      "RMSE: 1.0027\n",
      "MAE: 0.7919\n",
      "Epoch 34/50, Training Loss: 0.4631, Validation RMSE: 1.0027, Validation MAE: 0.7919\n",
      "\n",
      "Epoch 34 Metrics:\n",
      "  Time: 33.15s\n",
      "  Memory: 637.43MB\n",
      "  CPU: 50.6%\n",
      "  Loss: 0.4552\n",
      "  RMSE: 1.0013\n",
      "  MAE: 0.7911\n",
      "Epoch 34 - Emisiones: 0.00001746 kg, Acumulado: 0.00064728 kg, Loss: 0.4552\n",
      "RMSE: 1.0013\n",
      "MAE: 0.7911\n",
      "Epoch 35/50, Training Loss: 0.4552, Validation RMSE: 1.0013, Validation MAE: 0.7911\n",
      "\n",
      "Epoch 35 Metrics:\n",
      "  Time: 33.05s\n",
      "  Memory: 637.71MB\n",
      "  CPU: 51.9%\n",
      "  Loss: 0.4469\n",
      "  RMSE: 1.0074\n",
      "  MAE: 0.7940\n",
      "Epoch 35 - Emisiones: 0.00001742 kg, Acumulado: 0.00066470 kg, Loss: 0.4469\n",
      "RMSE: 1.0074\n",
      "MAE: 0.7940\n",
      "Epoch 36/50, Training Loss: 0.4469, Validation RMSE: 1.0074, Validation MAE: 0.7940\n",
      "\n",
      "Epoch 36 Metrics:\n",
      "  Time: 32.91s\n",
      "  Memory: 638.11MB\n",
      "  CPU: 51.9%\n",
      "  Loss: 0.4403\n",
      "  RMSE: 1.0088\n",
      "  MAE: 0.7941\n",
      "Epoch 36 - Emisiones: 0.00001731 kg, Acumulado: 0.00068202 kg, Loss: 0.4403\n",
      "RMSE: 1.0088\n",
      "MAE: 0.7941\n",
      "Epoch 37/50, Training Loss: 0.4403, Validation RMSE: 1.0088, Validation MAE: 0.7941\n",
      "\n",
      "Epoch 37 Metrics:\n",
      "  Time: 32.84s\n",
      "  Memory: 638.36MB\n",
      "  CPU: 51.4%\n",
      "  Loss: 0.4327\n",
      "  RMSE: 1.0101\n",
      "  MAE: 0.7971\n",
      "Epoch 37 - Emisiones: 0.00001728 kg, Acumulado: 0.00069930 kg, Loss: 0.4327\n",
      "RMSE: 1.0101\n",
      "MAE: 0.7971\n",
      "Epoch 38/50, Training Loss: 0.4327, Validation RMSE: 1.0101, Validation MAE: 0.7971\n",
      "\n",
      "Epoch 38 Metrics:\n",
      "  Time: 33.01s\n",
      "  Memory: 638.66MB\n",
      "  CPU: 51.1%\n",
      "  Loss: 0.4262\n",
      "  RMSE: 1.0147\n",
      "  MAE: 0.7994\n",
      "Epoch 38 - Emisiones: 0.00001740 kg, Acumulado: 0.00071669 kg, Loss: 0.4262\n",
      "RMSE: 1.0147\n",
      "MAE: 0.7994\n",
      "Epoch 39/50, Training Loss: 0.4262, Validation RMSE: 1.0147, Validation MAE: 0.7994\n",
      "\n",
      "Epoch 39 Metrics:\n",
      "  Time: 33.30s\n",
      "  Memory: 639.20MB\n",
      "  CPU: 51.1%\n",
      "  Loss: 0.4186\n",
      "  RMSE: 1.0170\n",
      "  MAE: 0.8029\n",
      "Epoch 39 - Emisiones: 0.00001757 kg, Acumulado: 0.00073426 kg, Loss: 0.4186\n",
      "RMSE: 1.0170\n",
      "MAE: 0.8029\n",
      "Epoch 40/50, Training Loss: 0.4186, Validation RMSE: 1.0170, Validation MAE: 0.8029\n",
      "\n",
      "Epoch 40 Metrics:\n",
      "  Time: 32.78s\n",
      "  Memory: 639.45MB\n",
      "  CPU: 51.3%\n",
      "  Loss: 0.4125\n",
      "  RMSE: 1.0200\n",
      "  MAE: 0.8033\n",
      "Epoch 40 - Emisiones: 0.00001724 kg, Acumulado: 0.00075150 kg, Loss: 0.4125\n",
      "RMSE: 1.0200\n",
      "MAE: 0.8033\n",
      "Epoch 41/50, Training Loss: 0.4125, Validation RMSE: 1.0200, Validation MAE: 0.8033\n",
      "\n",
      "Epoch 41 Metrics:\n",
      "  Time: 32.03s\n",
      "  Memory: 639.45MB\n",
      "  CPU: 50.8%\n",
      "  Loss: 0.4063\n",
      "  RMSE: 1.0241\n",
      "  MAE: 0.8051\n",
      "Epoch 41 - Emisiones: 0.00001681 kg, Acumulado: 0.00076831 kg, Loss: 0.4063\n",
      "RMSE: 1.0241\n",
      "MAE: 0.8051\n",
      "Epoch 42/50, Training Loss: 0.4063, Validation RMSE: 1.0241, Validation MAE: 0.8051\n",
      "\n",
      "Epoch 42 Metrics:\n",
      "  Time: 30.99s\n",
      "  Memory: 639.45MB\n",
      "  CPU: 51.3%\n",
      "  Loss: 0.3998\n",
      "  RMSE: 1.0250\n",
      "  MAE: 0.8096\n",
      "Epoch 42 - Emisiones: 0.00001614 kg, Acumulado: 0.00078445 kg, Loss: 0.3998\n",
      "RMSE: 1.0250\n",
      "MAE: 0.8096\n",
      "Epoch 43/50, Training Loss: 0.3998, Validation RMSE: 1.0250, Validation MAE: 0.8096\n",
      "\n",
      "Epoch 43 Metrics:\n",
      "  Time: 31.81s\n",
      "  Memory: 639.70MB\n",
      "  CPU: 48.7%\n",
      "  Loss: 0.3942\n",
      "  RMSE: 1.0293\n",
      "  MAE: 0.8115\n",
      "Epoch 43 - Emisiones: 0.00001666 kg, Acumulado: 0.00080111 kg, Loss: 0.3942\n",
      "RMSE: 1.0293\n",
      "MAE: 0.8115\n",
      "Epoch 44/50, Training Loss: 0.3942, Validation RMSE: 1.0293, Validation MAE: 0.8115\n",
      "\n",
      "Epoch 44 Metrics:\n",
      "  Time: 31.18s\n",
      "  Memory: 638.16MB\n",
      "  CPU: 51.5%\n",
      "  Loss: 0.3882\n",
      "  RMSE: 1.0313\n",
      "  MAE: 0.8124\n",
      "Epoch 44 - Emisiones: 0.00001624 kg, Acumulado: 0.00081735 kg, Loss: 0.3882\n",
      "RMSE: 1.0313\n",
      "MAE: 0.8124\n",
      "Epoch 45/50, Training Loss: 0.3882, Validation RMSE: 1.0313, Validation MAE: 0.8124\n",
      "\n",
      "Epoch 45 Metrics:\n",
      "  Time: 30.84s\n",
      "  Memory: 638.41MB\n",
      "  CPU: 48.6%\n",
      "  Loss: 0.3828\n",
      "  RMSE: 1.0323\n",
      "  MAE: 0.8140\n",
      "Epoch 45 - Emisiones: 0.00001604 kg, Acumulado: 0.00083339 kg, Loss: 0.3828\n",
      "RMSE: 1.0323\n",
      "MAE: 0.8140\n",
      "Epoch 46/50, Training Loss: 0.3828, Validation RMSE: 1.0323, Validation MAE: 0.8140\n",
      "\n",
      "Epoch 46 Metrics:\n",
      "  Time: 31.16s\n",
      "  Memory: 638.67MB\n",
      "  CPU: 48.5%\n",
      "  Loss: 0.3773\n",
      "  RMSE: 1.0358\n",
      "  MAE: 0.8150\n",
      "Epoch 46 - Emisiones: 0.00001623 kg, Acumulado: 0.00084962 kg, Loss: 0.3773\n",
      "RMSE: 1.0358\n",
      "MAE: 0.8150\n",
      "Epoch 47/50, Training Loss: 0.3773, Validation RMSE: 1.0358, Validation MAE: 0.8150\n",
      "\n",
      "Epoch 47 Metrics:\n",
      "  Time: 33.30s\n",
      "  Memory: 639.03MB\n",
      "  CPU: 48.7%\n",
      "  Loss: 0.3720\n",
      "  RMSE: 1.0383\n",
      "  MAE: 0.8186\n",
      "Epoch 47 - Emisiones: 0.00001757 kg, Acumulado: 0.00086719 kg, Loss: 0.3720\n",
      "RMSE: 1.0383\n",
      "MAE: 0.8186\n",
      "Epoch 48/50, Training Loss: 0.3720, Validation RMSE: 1.0383, Validation MAE: 0.8186\n",
      "\n",
      "Epoch 48 Metrics:\n",
      "  Time: 33.40s\n",
      "  Memory: 639.54MB\n",
      "  CPU: 53.7%\n",
      "  Loss: 0.3670\n",
      "  RMSE: 1.0404\n",
      "  MAE: 0.8202\n",
      "Epoch 48 - Emisiones: 0.00001764 kg, Acumulado: 0.00088483 kg, Loss: 0.3670\n",
      "RMSE: 1.0404\n",
      "MAE: 0.8202\n",
      "Epoch 49/50, Training Loss: 0.3670, Validation RMSE: 1.0404, Validation MAE: 0.8202\n",
      "\n",
      "Epoch 49 Metrics:\n",
      "  Time: 33.43s\n",
      "  Memory: 640.04MB\n",
      "  CPU: 51.6%\n",
      "  Loss: 0.3624\n",
      "  RMSE: 1.0424\n",
      "  MAE: 0.8219\n",
      "Epoch 49 - Emisiones: 0.00001764 kg, Acumulado: 0.00090246 kg, Loss: 0.3624\n",
      "RMSE: 1.0424\n",
      "MAE: 0.8219\n",
      "Epoch 50/50, Training Loss: 0.3624, Validation RMSE: 1.0424, Validation MAE: 0.8219\n",
      "\n",
      "Evaluando modelo en conjunto de prueba final...\n",
      "\n",
      "Configuración de historiales de usuario y similitudes entre películas\n",
      "Construyendo historiales de usuario...\n",
      "Construido historial para 6040 usuarios\n",
      "Calculando similitudes entre películas...\n",
      "Calculando similitudes para 3683 películas válidas...\n",
      "  Procesando lote 1/37...\n",
      "  Procesando lote 11/37...\n",
      "  Procesando lote 21/37...\n",
      "  Procesando lote 31/37...\n",
      "Similitudes calculadas\n",
      "\n",
      "Calculando RMSE con ajuste de historial\n",
      "\n",
      "Ejemplos de predicciones:\n",
      "Usuario 1012, Película 890: Real=5.00, NCF=4.18, Ajustada=4.10\n",
      "\n",
      "Demostrando predicción para Usuario 1012, Película 890:\n",
      "Predicción NCF: 4.1816\n",
      "Usuario ha calificado 39 películas\n",
      "\n",
      "Películas similares encontradas en historial:\n",
      "ID PelículaValoraciónSimilitud      \n",
      "849       5.00      0.3288         \n",
      "850       4.00      0.2967         \n",
      "2872      4.00      0.2681         \n",
      "2846      4.00      0.2410         \n",
      "860       5.00      0.2318         \n",
      "\n",
      "Predicción basada en historial: 3.9056\n",
      "Predicción final ponderada (0.3 peso historial): 4.0988\n",
      "Usuario 1012, Película 1900: Real=4.00, NCF=3.96, Ajustada=3.92\n",
      "Usuario 1012, Película 1099: Real=4.00, NCF=4.08, Ajustada=4.00\n",
      "Usuario 1012, Película 1131: Real=4.00, NCF=3.73, Ajustada=3.76\n",
      "Usuario 1012, Película 2197: Real=5.00, NCF=4.04, Ajustada=3.97\n",
      "\n",
      "Generando métricas finales del sistema con SystemMetricsTracker...\n",
      "\n",
      "=== Final Training Metrics ===\n",
      "Epoch 0: Time=47.11s, Memory=594.14MB, CPU=46.3%, Loss=1.3755, RMSE=0.9598, MAE=0.7659\n",
      "Epoch 1: Time=43.01s, Memory=633.51MB, CPU=70.5%, Loss=0.8670, RMSE=0.9346, MAE=0.7432\n",
      "Epoch 2: Time=34.29s, Memory=635.03MB, CPU=72.1%, Loss=0.8276, RMSE=0.9297, MAE=0.7353\n",
      "Epoch 3: Time=34.85s, Memory=636.88MB, CPU=54.9%, Loss=0.8087, RMSE=0.9277, MAE=0.7357\n",
      "Epoch 4: Time=35.14s, Memory=638.32MB, CPU=54.8%, Loss=0.7948, RMSE=0.9278, MAE=0.7345\n",
      "Epoch 5: Time=34.34s, Memory=638.82MB, CPU=59.6%, Loss=0.7838, RMSE=0.9263, MAE=0.7305\n",
      "Epoch 6: Time=34.77s, Memory=639.31MB, CPU=54.5%, Loss=0.7728, RMSE=0.9256, MAE=0.7333\n",
      "Epoch 7: Time=34.19s, Memory=639.82MB, CPU=54.4%, Loss=0.7618, RMSE=0.9268, MAE=0.7330\n",
      "Epoch 8: Time=34.51s, Memory=640.12MB, CPU=54.6%, Loss=0.7504, RMSE=0.9265, MAE=0.7338\n",
      "Epoch 9: Time=34.93s, Memory=640.64MB, CPU=57.0%, Loss=0.7385, RMSE=0.9282, MAE=0.7362\n",
      "Epoch 10: Time=37.08s, Memory=641.14MB, CPU=56.1%, Loss=0.7263, RMSE=0.9302, MAE=0.7331\n",
      "Epoch 11: Time=33.68s, Memory=641.70MB, CPU=62.1%, Loss=0.7137, RMSE=0.9344, MAE=0.7337\n",
      "Epoch 12: Time=36.14s, Memory=641.95MB, CPU=54.0%, Loss=0.7009, RMSE=0.9338, MAE=0.7370\n",
      "Epoch 13: Time=36.31s, Memory=642.46MB, CPU=59.3%, Loss=0.6874, RMSE=0.9355, MAE=0.7392\n",
      "Epoch 14: Time=35.49s, Memory=642.97MB, CPU=62.1%, Loss=0.6739, RMSE=0.9374, MAE=0.7399\n",
      "Epoch 15: Time=36.84s, Memory=643.42MB, CPU=60.2%, Loss=0.6609, RMSE=0.9397, MAE=0.7422\n",
      "Epoch 16: Time=45.69s, Memory=643.66MB, CPU=58.2%, Loss=0.6475, RMSE=0.9423, MAE=0.7465\n",
      "Epoch 17: Time=37.83s, Memory=643.92MB, CPU=75.0%, Loss=0.6344, RMSE=0.9460, MAE=0.7468\n",
      "Epoch 18: Time=32.66s, Memory=644.17MB, CPU=65.5%, Loss=0.6215, RMSE=0.9535, MAE=0.7485\n",
      "Epoch 19: Time=32.91s, Memory=644.68MB, CPU=51.3%, Loss=0.6085, RMSE=0.9530, MAE=0.7523\n",
      "Epoch 20: Time=33.58s, Memory=645.17MB, CPU=51.5%, Loss=0.5961, RMSE=0.9548, MAE=0.7547\n",
      "Epoch 21: Time=32.57s, Memory=645.85MB, CPU=52.8%, Loss=0.5837, RMSE=0.9585, MAE=0.7556\n",
      "Epoch 22: Time=33.04s, Memory=646.45MB, CPU=50.2%, Loss=0.5720, RMSE=0.9633, MAE=0.7578\n",
      "Epoch 23: Time=32.99s, Memory=646.70MB, CPU=52.6%, Loss=0.5606, RMSE=0.9657, MAE=0.7635\n",
      "Epoch 24: Time=33.43s, Memory=646.95MB, CPU=52.2%, Loss=0.5491, RMSE=0.9693, MAE=0.7667\n",
      "Epoch 25: Time=33.41s, Memory=647.45MB, CPU=52.2%, Loss=0.5378, RMSE=0.9739, MAE=0.7692\n",
      "Epoch 26: Time=33.19s, Memory=633.83MB, CPU=53.8%, Loss=0.5277, RMSE=0.9819, MAE=0.7706\n",
      "Epoch 27: Time=31.56s, Memory=635.16MB, CPU=51.8%, Loss=0.5175, RMSE=0.9817, MAE=0.7767\n",
      "Epoch 28: Time=31.08s, Memory=635.57MB, CPU=49.9%, Loss=0.5077, RMSE=0.9830, MAE=0.7775\n",
      "Epoch 29: Time=31.14s, Memory=635.82MB, CPU=49.0%, Loss=0.4978, RMSE=0.9861, MAE=0.7773\n",
      "Epoch 30: Time=30.94s, Memory=636.15MB, CPU=49.1%, Loss=0.4889, RMSE=0.9897, MAE=0.7814\n",
      "Epoch 31: Time=32.01s, Memory=636.40MB, CPU=48.7%, Loss=0.4802, RMSE=0.9940, MAE=0.7821\n",
      "Epoch 32: Time=33.20s, Memory=636.65MB, CPU=51.8%, Loss=0.4714, RMSE=0.9990, MAE=0.7860\n",
      "Epoch 33: Time=32.73s, Memory=636.91MB, CPU=51.5%, Loss=0.4631, RMSE=1.0027, MAE=0.7919\n",
      "Epoch 34: Time=33.15s, Memory=637.43MB, CPU=50.6%, Loss=0.4552, RMSE=1.0013, MAE=0.7911\n",
      "Epoch 35: Time=33.05s, Memory=637.71MB, CPU=51.9%, Loss=0.4469, RMSE=1.0074, MAE=0.7940\n",
      "Epoch 36: Time=32.91s, Memory=638.11MB, CPU=51.9%, Loss=0.4403, RMSE=1.0088, MAE=0.7941\n",
      "Epoch 37: Time=32.84s, Memory=638.36MB, CPU=51.4%, Loss=0.4327, RMSE=1.0101, MAE=0.7971\n",
      "Epoch 38: Time=33.01s, Memory=638.66MB, CPU=51.1%, Loss=0.4262, RMSE=1.0147, MAE=0.7994\n",
      "Epoch 39: Time=33.30s, Memory=639.20MB, CPU=51.1%, Loss=0.4186, RMSE=1.0170, MAE=0.8029\n",
      "Epoch 40: Time=32.78s, Memory=639.45MB, CPU=51.3%, Loss=0.4125, RMSE=1.0200, MAE=0.8033\n",
      "Epoch 41: Time=32.03s, Memory=639.45MB, CPU=50.8%, Loss=0.4063, RMSE=1.0241, MAE=0.8051\n",
      "Epoch 42: Time=30.99s, Memory=639.45MB, CPU=51.3%, Loss=0.3998, RMSE=1.0250, MAE=0.8096\n",
      "Epoch 43: Time=31.81s, Memory=639.70MB, CPU=48.7%, Loss=0.3942, RMSE=1.0293, MAE=0.8115\n",
      "Epoch 44: Time=31.18s, Memory=638.16MB, CPU=51.5%, Loss=0.3882, RMSE=1.0313, MAE=0.8124\n",
      "Epoch 45: Time=30.84s, Memory=638.41MB, CPU=48.6%, Loss=0.3828, RMSE=1.0323, MAE=0.8140\n",
      "Epoch 46: Time=31.16s, Memory=638.67MB, CPU=48.5%, Loss=0.3773, RMSE=1.0358, MAE=0.8150\n",
      "Epoch 47: Time=33.30s, Memory=639.03MB, CPU=48.7%, Loss=0.3720, RMSE=1.0383, MAE=0.8186\n",
      "Epoch 48: Time=33.40s, Memory=639.54MB, CPU=53.7%, Loss=0.3670, RMSE=1.0404, MAE=0.8202\n",
      "Epoch 49: Time=33.43s, Memory=640.04MB, CPU=51.6%, Loss=0.3624, RMSE=1.0424, MAE=0.8219\n",
      "\n",
      "=== Final Test Metrics ===\n",
      "Total Time: 2631.94s (Test: 918.68s)\n",
      "Final Memory: 634.65MB\n",
      "Final CPU: 44.6%\n",
      "Test RMSE: 0.9684\n",
      "Test MAE: 0.7662\n",
      "\n",
      "Generando gráficos y métricas de emisiones...\n",
      "\n",
      "Total CO2 Emissions: 0.001621 kg\n",
      "Métricas de emisiones guardadas en: results/emissions_reports/emissions_metrics_NCF_20250424-162236.csv\n",
      "Gráfico guardado en: results/emissions_plots/cumulative_emissions_vs_rmse_NCF_20250424-162236.png\n",
      "Gráfico guardado en: results/emissions_plots/metrics_by_epoch_NCF_20250424-162236.png\n",
      "Gráfico guardado en: results/emissions_plots/cumulative_emissions_performance_scatter_NCF_20250424-162236.png\n",
      "Gráfico comparativo guardado en: results/emissions_plots/rmse_vs_mae_NCF_20250424-162236.png\n",
      "Métricas del modelo guardadas en: results/model_metrics_20250424-162238.csv\n",
      "\n",
      "============================================================\n",
      "MÉTRICAS FINALES DEL SISTEMA (MEDICIÓN INDEPENDIENTE GARANTIZADA)\n",
      "============================================================\n",
      "Memoria final: 653.89 MB\n",
      "CPU final: 32.60%\n",
      "Tiempo total de ejecución: 2631.58 segundos\n",
      "RMSE final: 0.9684\n",
      "MAE final: 0.7662\n",
      "============================================================\n",
      "Métricas finales guardadas en: results/final_metrics_20250424-162238.csv\n",
      "\n",
      "Entrenamiento finalizado!\n",
      "Métricas finales - RMSE: 0.9684, MAE: 0.7662\n",
      "Programa finalizado correctamente.\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_squared_error = 0\n",
    "    total_absolute_error = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for user, movie, rating in loader:\n",
    "            output = model(user, movie)\n",
    "            loss = criterion(output, rating)\n",
    "            total_loss += loss.item() * len(rating)\n",
    "            \n",
    "            # Calculate squared error and absolute error\n",
    "            squared_error = (output - rating) ** 2\n",
    "            absolute_error = torch.abs(output - rating)\n",
    "            \n",
    "            # Accumulate total errors\n",
    "            total_squared_error += squared_error.sum().item()\n",
    "            total_absolute_error += absolute_error.sum().item()\n",
    "            total_samples += len(rating)\n",
    "    \n",
    "    # Compute metrics\n",
    "    avg_loss = total_loss / total_samples\n",
    "    rmse = np.sqrt(total_squared_error / total_samples)\n",
    "    mae = total_absolute_error / total_samples\n",
    "    \n",
    "    return avg_loss, rmse, mae\n",
    "\n",
    "def train_model():\n",
    "    # Inicializar trackers\n",
    "    print(\"Inicializando trackers...\")\n",
    "    system_tracker = SystemMetricsTracker()\n",
    "    emissions_tracker = EmissionsPerEpochTracker(result_path, \"NCF\")\n",
    "    \n",
    "    # Inicializar listas para seguimiento de métricas\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_rmses = []\n",
    "    val_maes = []\n",
    "    \n",
    "    # Guardar tiempo de inicio para medir tiempo total\n",
    "    tiempo_inicio = time.time()\n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"\\nComenzando entrenamiento ({config['epochs']} épocas)...\")\n",
    "    for epoch in range(config['epochs']):\n",
    "        # Iniciar seguimiento de época\n",
    "        system_tracker.start_epoch(epoch)\n",
    "        emissions_tracker.start_epoch(epoch)\n",
    "        \n",
    "        # Cambiar a modo de entrenamiento\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for user, movie, rating in train_loader:\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(user, movie)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(output, rating)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * len(rating)\n",
    "            total_samples += len(rating)\n",
    "        \n",
    "        # Calcular loss promedio de esta época\n",
    "        avg_train_loss = total_loss / total_samples\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Evaluar en conjunto de validación\n",
    "        val_loss, val_rmse, val_mae = calculate_metrics(model, val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        val_rmses.append(val_rmse)\n",
    "        val_maes.append(val_mae)\n",
    "        \n",
    "        # Actualizar trackers\n",
    "        system_tracker.end_epoch(epoch, avg_train_loss, val_rmse, val_mae)\n",
    "        emissions_tracker.end_epoch(epoch, avg_train_loss, val_rmse, val_mae)\n",
    "        \n",
    "        # Mostrar progreso\n",
    "        if (epoch + 1) % config['display_step'] == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{config['epochs']}, Training Loss: {avg_train_loss:.4f}, Validation RMSE: {val_rmse:.4f}, Validation MAE: {val_mae:.4f}\")\n",
    "    \n",
    "    print(\"\\nEvaluando modelo en conjunto de prueba final...\")\n",
    "    system_tracker.start_epoch(\"test\")\n",
    "    \n",
    "    # Preparar historial después del entrenamiento\n",
    "    if isinstance(model, NCFWithHistory):\n",
    "        print(\"\\nConfiguración de historiales de usuario y similitudes entre películas\")\n",
    "        model.set_user_histories(train_df)\n",
    "        model.compute_item_similarities(train_df)\n",
    "        \n",
    "        # Evaluación final con ajuste de historial\n",
    "        print(\"\\nCalculando RMSE con ajuste de historial\")\n",
    "        total_squared_error = 0\n",
    "        total_absolute_error = 0\n",
    "        total_samples = 0\n",
    "        examples = []\n",
    "        \n",
    "        # Muestrear hasta 5 ejemplos para mostrar\n",
    "        sample_users = np.random.choice(n_users, min(10, n_users), replace=False)\n",
    "        \n",
    "        for user_id in range(n_users):\n",
    "            user_test_data = val_df[val_df['userId'] == user_id]\n",
    "            if len(user_test_data) == 0:\n",
    "                continue\n",
    "                \n",
    "            for _, row in user_test_data.iterrows():\n",
    "                movie_id = row['movieId']\n",
    "                actual_rating = row['rating']\n",
    "                \n",
    "                # Obtener predicciones\n",
    "                ncf_pred, adjusted_pred = model.predict_for_user(user_id, movie_id)\n",
    "                \n",
    "                # Calcular errores\n",
    "                squared_error = (adjusted_pred - actual_rating) ** 2\n",
    "                absolute_error = abs(adjusted_pred - actual_rating)\n",
    "                \n",
    "                total_squared_error += squared_error\n",
    "                total_absolute_error += absolute_error\n",
    "                total_samples += 1\n",
    "                \n",
    "                # Guardar algunos ejemplos para mostrar\n",
    "                if user_id in sample_users and len(examples) < 5:\n",
    "                    examples.append((user_id, movie_id, actual_rating, ncf_pred, adjusted_pred))\n",
    "        \n",
    "        # Calcular métricas finales\n",
    "        final_rmse = np.sqrt(total_squared_error / total_samples)\n",
    "        final_mae = total_absolute_error / total_samples\n",
    "        \n",
    "        # Mostrar ejemplos\n",
    "        print(\"\\nEjemplos de predicciones:\")\n",
    "        for user_id, movie_id, actual, ncf_pred, adj_pred in examples:\n",
    "            print(f\"Usuario {user_id}, Película {movie_id}: Real={actual:.2f}, NCF={ncf_pred:.2f}, Ajustada={adj_pred:.2f}\")\n",
    "            # Mostrar detalles de uno de los ejemplos\n",
    "            if examples.index((user_id, movie_id, actual, ncf_pred, adj_pred)) == 0:\n",
    "                model.demonstrate_prediction(user_id, movie_id)\n",
    "    else:\n",
    "        # Evaluación estándar para modelo NCF normal\n",
    "        final_loss, final_rmse, final_mae = calculate_metrics(model, val_loader)\n",
    "    \n",
    "    # Finalizar seguimiento - Asegurarnos que esto se ejecute sin excepciones\n",
    "    metrics_displayed = False\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nGenerando métricas finales del sistema con SystemMetricsTracker...\")\n",
    "        system_tracker.end_test(final_rmse, final_mae)\n",
    "        metrics_displayed = True\n",
    "    except Exception as e:\n",
    "        print(f\"Error al generar métricas finales con tracker: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nGenerando gráficos y métricas de emisiones...\")\n",
    "        emissions_tracker.end_training(final_rmse, final_mae)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al generar métricas de emisiones: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Guardar métricas de entrenamiento\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'epoch': list(range(config['epochs'])),\n",
    "        'train_loss': train_losses,\n",
    "        'val_loss': val_losses,\n",
    "        'val_rmse': val_rmses,\n",
    "        'val_mae': val_maes\n",
    "    })\n",
    "    \n",
    "    metrics_file = f\"{result_path}/model_metrics_{timestamp}.csv\"\n",
    "    metrics_df.to_csv(metrics_file, index=False)\n",
    "    print(f\"Métricas del modelo guardadas en: {metrics_file}\")\n",
    "    \n",
    "    # Siempre mostrar las métricas finales, incluso si los otros métodos fallaron\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MÉTRICAS FINALES DEL SISTEMA (MEDICIÓN INDEPENDIENTE GARANTIZADA)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Mediciones directas\n",
    "    memoria_final = psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2\n",
    "    cpu_final = psutil.cpu_percent(interval=1.0)  # Medición de 1 segundo\n",
    "    tiempo_total = time.time() - tiempo_inicio\n",
    "    \n",
    "    print(f\"Memoria final: {memoria_final:.2f} MB\")\n",
    "    print(f\"CPU final: {cpu_final:.2f}%\")\n",
    "    print(f\"Tiempo total de ejecución: {tiempo_total:.2f} segundos\")\n",
    "    print(f\"RMSE final: {final_rmse:.4f}\")\n",
    "    print(f\"MAE final: {final_mae:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Guardar las métricas finales en un archivo separado para asegurar que se capturen\n",
    "    final_metrics = {\n",
    "        'final_memory_mb': memoria_final,\n",
    "        'final_cpu_percent': cpu_final,\n",
    "        'total_time_sec': tiempo_total,\n",
    "        'final_rmse': final_rmse,\n",
    "        'final_mae': final_mae,\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    \n",
    "    final_metrics_df = pd.DataFrame([final_metrics])\n",
    "    final_metrics_file = f\"{result_path}/final_metrics_{timestamp}.csv\"\n",
    "    final_metrics_df.to_csv(final_metrics_file, index=False)\n",
    "    print(f\"Métricas finales guardadas en: {final_metrics_file}\")\n",
    "    \n",
    "    print(f\"\\nEntrenamiento finalizado!\")\n",
    "    print(f\"Métricas finales - RMSE: {final_rmse:.4f}, MAE: {final_mae:.4f}\")\n",
    "    \n",
    "    return final_rmse, final_mae\n",
    "\n",
    "# Preparar los datos\n",
    "print(\"\\nPreparando los datasets...\")\n",
    "train_df, val_df = train_test_split(ratings_df, test_size=config['test_size'], random_state=config['random_state'])\n",
    "\n",
    "# Crear datasets y dataloaders\n",
    "train_dataset = MovieLensDataset(train_df)\n",
    "val_dataset = MovieLensDataset(val_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "print(f\"Datos preparados: {len(train_df)} muestras de entrenamiento, {len(val_df)} muestras de validación\")\n",
    "\n",
    "\n",
    "# Inicializar modelo con soporte de historial\n",
    "model = NCFWithHistory(n_users, n_movies, \n",
    "                     embedding_dim=config['embedding_dim'], \n",
    "                     hidden_dim=config['hidden_dim'],\n",
    "                     history_weight=0.3)  # Ajustar este valor según sea necesario\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "# Ejecutar entrenamiento\n",
    "final_rmse, final_mae = train_model()\n",
    "\n",
    "print(\"Programa finalizado correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8206147a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probando predicción para usuario y película específicos\n",
      "\n",
      "Prediciendo valoración para usuario 552, película 856\n",
      "\n",
      "Demostrando predicción para Usuario 552, Película 856:\n",
      "Predicción NCF: 3.8491\n",
      "Usuario ha calificado 47 películas\n",
      "\n",
      "Películas similares encontradas en historial:\n",
      "ID PelículaValoraciónSimilitud      \n",
      "573       4.00      0.1480         \n",
      "361       4.00      0.1460         \n",
      "786       5.00      0.1398         \n",
      "572       5.00      0.1384         \n",
      "2857      5.00      0.1316         \n",
      "\n",
      "Predicción basada en historial: 4.4488\n",
      "Predicción final ponderada (0.3 peso historial): 4.0290\n",
      "\n",
      "Resumen de predicción:\n",
      "Predicción NCF: 3.8491\n",
      "Predicción ajustada con historial: 4.0290\n"
     ]
    }
   ],
   "source": [
    "def predict_rating_for_user_and_movie(user_id, movie_id):\n",
    "    \"\"\"\n",
    "    Predecir la valoración que un usuario daría a una película específica,\n",
    "    utilizando el modelo NCF ajustado con historial.\n",
    "    \"\"\"\n",
    "    if not isinstance(model, NCFWithHistory):\n",
    "        raise TypeError(\"Esta función requiere un modelo NCFWithHistory\")\n",
    "    \n",
    "    # Verificar que el usuario y la película existan en nuestros datos\n",
    "    if user_id >= n_users or user_id < 0:\n",
    "        raise ValueError(f\"ID de usuario inválido. Debe estar entre 0 y {n_users-1}\")\n",
    "    \n",
    "    if movie_id >= n_movies or movie_id < 0:\n",
    "        raise ValueError(f\"ID de película inválido. Debe estar entre 0 y {n_movies-1}\")\n",
    "    \n",
    "    # Obtener predicciones\n",
    "    ncf_pred, adjusted_pred = model.predict_for_user(user_id, movie_id)\n",
    "    \n",
    "    # Mostrar detalles del proceso de predicción\n",
    "    model.demonstrate_prediction(user_id, movie_id)\n",
    "    \n",
    "    return ncf_pred, adjusted_pred\n",
    "\n",
    "# Ejemplo de uso\n",
    "print(\"\\nProbando predicción para usuario y película específicos\")\n",
    "try:\n",
    "    # Elegir un usuario y película aleatorios para probar\n",
    "    sample_user = np.random.randint(0, n_users)\n",
    "    sample_movie = np.random.randint(0, n_movies)\n",
    "    \n",
    "    print(f\"\\nPrediciendo valoración para usuario {sample_user}, película {sample_movie}\")\n",
    "    ncf_pred, history_pred = predict_rating_for_user_and_movie(sample_user, sample_movie)\n",
    "    print(f\"\\nResumen de predicción:\")\n",
    "    print(f\"Predicción NCF: {ncf_pred:.4f}\")\n",
    "    print(f\"Predicción ajustada con historial: {history_pred:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error al predecir: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
